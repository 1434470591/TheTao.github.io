\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage{tikz}
\usepackage{url}  % 或 \usepackage{hyperref}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{makecell}
% 必备宏包
\usepackage{booktabs}  % 必须：提供三线表 (\toprule, \midrule, \bottomrule)
\usepackage{amssymb}   % 必须：提供基础数学符号 (对钩和叉号)
\usepackage{multirow}  % 可选：用于合并行
\usepackage{xcolor}    % 可选：用于灰底高亮

% 定义安全版符号 (不再依赖 pifont)
\newcommand{\cmark}{\checkmark}     % 使用数学字体的对钩
\newcommand{\xmark}{$\times$}       % 使用数学字体的乘号作为叉
\newcommand{\nmark}{--}             % 或者用横线表示未选用
\usetikzlibrary{positioning, calc}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Spatial-Temporal Channel Prediction via a Dual-Guided VLM-based Framework}

\author{Anonymous ICME submission}

\maketitle

\begin{abstract}
Channel state information (CSI) prediction is a crucial technology for massive multiple-input multiple-output (MIMO) systems in future sixth-generation (6G) wireless communication networks. Existing advancements predominantly focus on spatial or temporal features in isolation, thereby failing to capture the inherent spatial-temporal correlations of high-dimensional CSI. In this paper, leveraging the superior capability of vision-language models (VLMs) in capturing spatial-temporal dependencies via multimodal representation learning, we propose a novel dual-guided VLM-based framework for spatial-temporal CSI prediction. The framework integrates a spatial-structural guidance block to extract geometric features from CSI data and a temporal-coherence guidance block to capture temporal continuity and physical consistency. Experimental results demonstrate that our proposed model outperforms the compared baseline methods in term of normalized mean squared error (NMSE) and squared generalized cosine similarity (SGCS), verifying its capability in capturing high-dimensional channel structures.

% \cite{cp-cvcnn}\cite{vit}\cite{cp-ar}\cite{cp-kf}

% This document is a model and instructions for \LaTeX.
% This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
% or Math in Paper Title or Abstract. The abstract should contain about 100 to 150 words, and should be identical to the abstract text submitted electronically. 
\end{abstract}

\begin{IEEEkeywords}
Channel prediction, spatial-temporal modeling, vision-language models, multimodal learning, coherence embedding
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
The accurate acquisition of channel state information (CSI) is pivotal in Massive Multiple-Input Multiple-Output(MIMO) communication systems~\cite{DBLP:books/cu/TV2005}, facilitating advanced transmission strategies, such as precoding, beamforming, and power allocation, thereby significantly improving system performance. 
Typically, as illustrated in Fig.~\ref{fig:data}, CSI is acquired via pilot symbols embedded in the transmission frame. However, in high-mobility scenarios, channel aging renders the channel information outdated, and increasing pilot density to mitigate this incurs severe overhead. Therefore, channel prediction, defined as inferring future channel information based on historical data, has become a key technology for enhancing system performance.

Driven by recent breakthroughs in deep learning (DL), artificial intelligence (AI) has significantly improved performance in wireless communications, such as channel estimation~\cite{cf-CEFnet}, channel feedback~\cite{cf-bi-ImCsiNet}, signal detection~\cite{detection}, and beamforming~\cite{ECT-Net}, while also encompassing channel prediction. Recurrent Neural Networks(RNNs) and Long Short-Term Memory(LSTM) have demonstrated strong performance in channel prediction by effectively capturing dynamic temporal dependencies~\cite{cp-rnn-1, cp-rnn-2}. Studies on temporal feature modeling of CSI have been further advanced by Transformers~\cite{cp-transformer} and Large Language Model (LLM)-based approaches~\cite{LLM4CP}, which excel at modeling long-range sequential correlations. In parallel, researchs including complex-valued convolution neural network (CVCNN)~\cite{cp-cvcnn} and STEM GNN~\cite{cp-stem-gnn} have demonstrated superior capability in modeling the spatial structure of wireless channels. WiFo~\cite{wifo} innovatively applies masked autoencoders (MAE)~\cite{MAE} to reconstruction tasks, treating CSI matrices as visual images to extract deep spatial structural features. 
\begin{figure}[tb]
\centerline{\includegraphics{./pic/Task.pdf}}
% \includegraphics[width=\linewidth]{./pic/Task.pdf}
\caption{The data structure in MIMO-OFDM system. Pilot symbols are predefined reference signals used for channel estimation, data symbols are unknown user information that must be detected based on the estimated channel.}
\label{fig:data}
\end{figure}

\begin{figure*}[tb]
\centerline{\includegraphics{./pic/Framework.pdf}}
% \includegraphics[width=\linewidth]{./pic/Overview.pdf}
\caption{Overview of proposed method.}
\label{fig:Overview}
\end{figure*}
% Despite these achievements, prior research fundamentally model the spatial and temporal domains in isolation, ignoring the spatiotemporal evolution inherent in wireless channels. To bridge this gap, we introduce a novel framework that use a pre-trained vision-language model to enhance channel prediction by jointly integrating visual, textual, and CSI modalities. Leveraging the strong cross-modal alignment capabilities of vision-language models,  effectively embeds CSI data into the shared visual-linguistic semantic space, thereby enabling seamless integration across the three modalities. This unified representation fosters cross-modal interactions and allows each modality to contribute complementary information. Specifically, complex-valued CSI data—comprising in-phase (I) and quadrature (Q) components—can be naturally organized into a two-dimensional matrix across multiple time steps and subcarriers, exhibiting a structural resemblance to visual data. Moreover, motivated by the observation that model like~\cite{TEST} maps time-series features into the embedding space of large language models for unified semantic modeling, coherence embedding, aligning CSI embeddings space with VLMs embeddings space, is adopted.

Despite these achievements, prior research fundamentally treats the spatial and temporal domains in isolation, ignoring the intrinsic spatiotemporal coupling of wireless channels. Specifically, temporal-centric approaches typically flatten high-dimensional CSI tensors into vectors, a process that inevitably obliterates the underlying spatial structure. Conversely, spatial-focused models, while preserving structural features, often struggle to maintain precision across long prediction windows due to limited temporal modeling capabilities. To bridge this gap, we introduce a dual-guided framework that leverages the pre-trained vision encoder and decoder-only LLMs to enhance spatial-temporal channel prediction. Our contributions are summarized as follows:

\begin{itemize}
    \item We propose VLM-based framework to enhance spatial-temporal channel prediction by leveraging the robust representational capabilities of pre-trained vision encoder and decoder-only LLMs.
    \item We employ a \textbf{Spatial-Structural Guidance} block and a \textbf{Temporal-coherence Guidance} block to capture the spatial-temporal feature of wireless channels in Massive MIMO system.
    \item Experimental results show that this method achieves superior performance on spatial-temporal channel prediction tasks, measured by NMSE and SGCS.
\end{itemize}

\section{Related work}
\label{sec:rew}

\subsection{Channel prediction (CP)}
Deep learning-based strategies have been increasingly applied to CP.
In particular, recurrent neural network, such as LSTM, excel in channel prediction by capturing dynamic temporal features~\cite{cp-rnn-1, cp-rnn-2}. Additionally, CNN-based approaches~\cite{cp-cnn-1, cp-cnn-2} employ convolutional neural networks(CNNs) with complex-valued convolutional layers to extract the spatial structure of CSI data. By facilitating parallel computation and paying attention to important patterns, Transformer-based models have significantly advanced channel prediction in challenging environments~\cite{cp-transformer}. Nevertheless, the lack of accurate modeling and robust generalization remains a key limitation of these models. More recently, inspired by the success of large language modals in fields of natural language processing (NLP)~\cite{gpt3}, some studies reflects their potential in CSI tasks. For instance, LLM4CP~\cite{LLM4CP} fine-tunes a pre-trained GPT-2 for CSI data and deploy a set of modules to boost model effectiveness. Similarly, method~\cite{LLM4CE} leverages the powerful noise removal capability of LLM to improve CSI reconstruction performance. However, these methods remain limited in their ability to align CSI data with the textual input required by LLMs and ignore the inherent structural similarities between CSI and computer vision (CV) data.

\subsection{Vision-Language Models (VLMs)} 
VLMs are fundamental to multimodal learning, enabling joint understanding of visual and textual modalities.
CLIP~\cite{clip} and ALIGN~\cite{align} demonstrate that contrastive learning effectively aligns image and text embeddings in a shared latent space. 
Studies such as Flamingo~\cite{flamingo} and BLIP~\cite{blip} further improve cross-modal interaction by incorporating cross-attention mechanism.
Beyond conventional computer vision, recent research~\cite{Time-VLM} has begun to extended the application of VLMs to non-visual domains, transforming structured data into visual representations and enabling the reuse of pre-trained visual backbones.
However, the exploration of VLMs in channel prediction is still in its infancy. These advances demonstrate that VLMs are not limited to native images and can serve as universal cross-modal learners across diverse and data-intensive tasks.
% Our goal is to bridge this gap by leveraging VLMs to combine CSI data, visual, and textual modalities. These advances demonstrate that VLMs are not limited to native images and can reinforce their potential to serve as universal cross-modal learners across diverse scientific and data-intensive tasks.


\section{Method}
\label{sec:method}

\subsection{Problem Formulation}
\label{subsec:problem_formulation}

As shown in Fig.~\ref{fig:data}, we consider a MIMO-OFDM system with $N_t$ transmit and $N_r$ receive antennas operating over $N_c$ subcarriers. At time step $t$ and subcarrier frequency $k \in \{1,\dots,N_c\}$, the received signal $\mathbf{y}_{t,k} \in \mathbb{C}^{N_r}$ is modeled as:
\begin{equation}
    \mathbf{y}_{t,k} = \mathbf{H}_{t,k} \mathbf{x}_{t,k} + \mathbf{n}_{t,k},
\label{eq:system_model}
\end{equation}
where $\mathbf{x}_{t,k} \in \mathbb{C}^{N_t}$ denotes the transmitted vector, and $\mathbf{n}_{t,k} \sim \mathcal{CN}(\mathbf{0}, \sigma^2\mathbf{I})$ represents the additive complex Gaussian noise. The term $\mathbf{H}_{t,k} \in \mathbb{C}^{N_r \times N_t}$ is the Channel Frequency Response (CFR) matrix, i.e., the channel state information (CSI). Aggregating the channel matrices over all subcarriers, we represent the CSI snapshot at time $t$ as
$\mathcal{H}_t \triangleq \{\mathbf{H}_{t,1},\dots,\mathbf{H}_{t,N_c}\}$.
Standard schemes insert dense pilots in each coherence interval to estimate $\mathcal{H}_t$, which incurs substantial overhead and reduces spectral efficiency in large-scale or fast-varying channels. To mitigate this burden, we %exploit the temporal correlation of $\{\mathcal{H}_t\}$
formulate channel prediction as a time-series forecasting task, where a model with parameters $\boldsymbol{\Theta}$ maps $P$ historical CSI snapshots to $L$ future ones:
\begin{equation}
    \big(\widetilde{\mathcal{H}}_{t+1},\dots,\widetilde{\mathcal{H}}_{t+L}\big)
    = f_{\boldsymbol{\Theta}}\big(\mathcal{H}_{t-P+1},\dots,\mathcal{H}_{t}\big),
\end{equation}
where $\widetilde{\mathcal{H}}_{t+\ell}$ denotes the predicted CSI at time index $t+\ell$, $\ell = 1,\dots,L$.

\subsection{Overall Architecture}

The overall architecture of our proposed model has been illustrated in Fig.~\ref{fig:Overview}, employing a VLM-based framework with forzen vision encoder and frozen decoder-only LLM. In order to enhance the extraction of spatiotemporal CSI features by the VLM, we introduce a dual-guidance mechanism: the Spatial-Structural Guidance Block and the Temporal-Coherence Guidance Block. The following sections provide a detailed description of each block.
% In order to improve the VLM’s extraction of CSI spatial-temporal features, we introduce a dual-guidance mechanism: spatial-structural guidance block and temporal-coherence guidance block. The following sections provide a detailed description of each block.

% These two modules independently extract features, SSG block utilizes the \textit{AR(2+1)D-CVC} to factorize spatial and temporal convolutions to extract geometric features, and the prompt-aware Quering Transformer, or Q-Former, to extract visual features from a frozen visual encoder. TCG block introduces the Coherence Embedding, unlike standard positional encodings, to capture the temporal continuity and physical consistency of the CSI sequence. Ultimately, the outputs of these two modules are fed into a frozen LLM for generative forecasting.

\begin{algorithm}[tb]
   \caption{Coherence Segmentation}
   \label{alg:Coherence}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Historical CSI sequence $\mathbf{X} \in \mathbb{C}^{T \times D}$, sensitivity threshold $\eta$ (Hyperparameter)
   \STATE {\bfseries Output:} Coherence Segment Indices $\mathbf{S} \in \mathbb{Z}^{T}$

   \vspace{0.1cm}

   \STATE $current\_id \leftarrow 0$
   \STATE $\mathbf{S}[1] \leftarrow current\_id$
   \FOR{$t=2$ {\bfseries to} $T$}
    %    \STATE \textcolor{gray}{\# Calculate change ratio relative to the preceding step}
        \STATE $\boldsymbol{\delta} \leftarrow \| \mathbf{X}_t - \mathbf{X}_{t-1} \|_2$ 
\vspace{0.10cm}
       \STATE $ratio \leftarrow \dfrac{\boldsymbol{\delta}}{ \| \mathbf{X}_{t-1} \|_2 + \epsilon}$
\vspace{0.10cm}
       \IF{$ratio < \eta $}
        %    \STATE \textcolor{gray}{\# Coherence maintained: Assign same ID}
           \STATE $\mathbf{S}[t] \leftarrow current\_id$
       \ELSE
        %    \STATE \textcolor{gray}{\# Coherence break detected: Increment ID}
           \STATE $\mathbf{S}[t] \leftarrow current\_id + 1$

           \STATE $current\_id \leftarrow current\_id + 1$
       \ENDIF
   \ENDFOR
   
   \RETURN $\mathbf{S}$
\end{algorithmic}
\end{algorithm}
\subsection{Spatial-Structural Guidance}

The objective of this block is to extract geometric features of CSI data and extract visual features from frozen vision encoder.
Following research \cite{cnn-before-vit} that proving early convolutions improve Visual Transformer(ViT) optimization, we first process the historical CSI $\mathcal{X}_{\mathrm{his}}=\{\mathcal{H}_{t-P+1},\ldots,\mathcal{H}_{t}\}\in\mathbb{C}^{P\times N_c\times N_r\times N_t}$ through residual (2+1) dimensional complex-valued convolutional layers before feeding it into the pre-trained vision encoder. We utilize a complex-valued convolutional neural network(CVCNN) instead of whole CNN to explicitly preserve phase information and factorize the standard 3D convolutional kernel size $3 \times 3 \times 3$ into $3 \times 3 \times 1$ and $1 \times 1 \times 3$ to effectively capture the spatial correlations of $\mathcal{X}_{\mathrm{hist}}$ between the transmit antennas and receive antennas and the local temporal dynamics in the $P$ dimension. 
% R(2+1)D \cite{R2+1D} factorizes the standard 3D convolutional filter into separate spatial and temporal components.ACNet \cite{ACNet} utilizing asymmetric convolution respectively.
% The operation can be formulated as:

We train Querying Transformer (Q-Former) from scratch. The input to the Q-Former contains a set of K learnable query embeddings $\mathbf{Q} \in \mathbb{R}^{K \times D_q}$ and the instruction-aware visual features from the output embeddings of the frozen vision encoder $\mathbf{F}_{ve}$. The query embeddings are initialized randomly and optimized during training. The instruction prompt $\mathbf{I}$ is designed to guide the Q-Former to extract visual features relevant to CSI prediction tasks.
Through $L=6$ stacked layers, the queries interact with the visual features to compress spatial information into instruction-aware visual representations. Each layer consists of a Multi-Head Self-Attention(MHSA) module, a Multi-Head Cross-Attention(MHCA) module, and a Feed-Forward Network(FFN). The update process for the $l$-th layer is formulated as:
\begin{align}
\tilde{\mathbf{Q}}^{(l)} &= \mathbf{Q}^{(l-1)} + \operatorname{MHSA}\Big(\operatorname{LN}([\mathbf{Q}^{(l-1)}, \mathbf{I}])\Big),\\
\hat{\mathbf{Q}}^{(l)} &= \tilde{\mathbf{Q}}^{(l)} + \operatorname{MHCA}\Big(\operatorname{LN}(\tilde{\mathbf{Q}}^{(l)}), \mathbf{F}_{ve}, \mathbf{F}_{ve}\Big),\\
\mathbf{Q}^{(l)} &= \hat{\mathbf{Q}}^{(l)} + \operatorname{FFN}\Big(\operatorname{LN}(\hat{\mathbf{Q}}^{(l)})\Big),
\end{align}
where $\operatorname{LN}(\cdot)$ denotes Layer Normalization, $\mathbf{I}$ denotes the instruction prompt, $\operatorname{FFN}$ consists of two linear layers with a ReLU activation: $\operatorname{FFN}(x)=\max(0, x\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2$, and $\mathbf{Q}^{(l)} $ represents the output of the $l$-th Q-Former layer.

% \begin{equation}
%     \mathbf{F}_{vv} = \operatorname{Q-Former}(\mathbf{Q}, \mathbf{F}_{ve}, prompt)
% \end{equation}
% These vectors $\mathbf{F}_{vv}$ serve as soft prompt input to the frozen LLM.

% \begin{algorithm}[tb]
%    \caption{Complex Rate-Adaptive Coherence Segmentation (CRACS)}
%    \label{alg:cracs}
% \begin{algorithmic}[1]
%    \STATE {\bfseries Input:} Complex CSI sequence $\mathbf{H} \in \mathbb{C}^{T \times D}$, Sensitivity threshold $\tau \in \mathbb{R}^+$
%    \STATE {\bfseries Output:} Coherence Segment Indices $\mathbf{S} \in \mathbb{Z}^{T}$
   
%    \vspace{0.1cm}
%    \STATE \textcolor{gray}{\# Step 1: Compute Complex Differential Magnitude (The Rate)}
%    \STATE Initialize rate vector $\boldsymbol{\rho} \in \mathbb{R}^{T}$
%    \FOR{$t=2$ {\bfseries to} $T$}
%        \STATE \textcolor{gray}{\# Euclidean distance in complex plane: $\sqrt{\Delta Re^2 + \Delta Im^2}$}
%        \STATE $\boldsymbol{\rho}_t \leftarrow \| \mathbf{H}_t - \mathbf{H}_{t-1} \|_2$
%    \ENDFOR
%    \STATE $\boldsymbol{\rho}_1 \leftarrow \boldsymbol{\rho}_2$ \hfill \textcolor{gray}{\# Padding for the first step}
   
%    \vspace{0.1cm}
%    \STATE \textcolor{gray}{\# Step 2: Backward Recursive Clustering}
%    \STATE Initialize $\mathbf{S}$ with zeros; $id \leftarrow 0$; $\mathbf{S}[T] \leftarrow 0$
   
%    \FOR{$t=T$ {\bfseries down to} $2$}
%        \STATE \textcolor{gray}{\# Calculate ratio of current rate to preceding rate}
%        \STATE $ratio \leftarrow \frac{\boldsymbol{\rho}_t}{\boldsymbol{\rho}_{t-1} + \epsilon}$
       
%        \IF{$ratio < \tau$}
%            \STATE \textcolor{gray}{\# Stable evolution detected: Maintain Coherence ID}
%            \STATE $\mathbf{S}[t-1] \leftarrow id$
%        \ELSE
%            \STATE \textcolor{gray}{\# Volatility spike detected: Assign New ID}
%            \STATE $id \leftarrow id + 1$
%            \STATE $\mathbf{S}[t-1] \leftarrow id$
%        \ENDIF
%    \ENDFOR
   
%    \RETURN $\mathbf{S}$
% \end{algorithmic}
% \end{algorithm}


\subsection{Temporal-coherence guidance block}
To be fed into frozen LLM model, we first reshape historical CSI $\mathcal {X}_{his}$ into dense real-valued form $\mathcal X_{rearrange}\in\mathbb{R}^{P\times (2 \cdot N_c \cdot N_r \cdot N_t)}$, where the real and imaginary parts of each complex entry are decomposed and concatenated, and then apply instance normalization \cite{norm} to standardize each sample to zero mean and unit variance.

We introduce the Coherence Embedding as the primary mechanism of this block, motivated by the theoretical channel correlation properties. The calculation method for coherence segmentation $\mathbf{S \in \mathbb{Z}^{T}}$ can be summarized by algorithm~\ref{alg:Coherence}.
% inspired by \cite{TcEmbedding}
We calculate the coherence embedding $\mathbf{E}_{coh} \in \mathbb{R}^{P \times D_e}$ by mapping the coherence segment indices $\mathbf{S}$ through a learnable embedding layer $\boldsymbol{Embedding}(\cdot)$, where $D_e$ denotes the coherence embedding dimension.

Thus, the composite embedding input $\mathbf{H}_{in}$ for frozen decoder-only LLM can be represented as:
\begin{equation}
    \mathbf{H}_{in} = [\mathbf{F}_{vv}, \mathbf{F}_{a} + \mathbf{E}_{coh}] + \mathbf{E}_{pos} + \mathbf{E}_{modal} ,
\end{equation}
where $\mathbf{E}_{modal}$ and $\mathbf{E}_{pos}$ denote standard modal-type and positional embeddings in \cite{Vilt}, and $\mathbf{E}_{coh}$ represents the coherence embedding.
% is a learnable coherence embedding vector designed to guide the frozen LLM in maintaining the temporal causality and physical consistency of the predicted CSI series.
The LLM processes these embeddings to generate output hidden states that encode future channel dynamics. To map these semantic representations back to the physical CSI space, we pass the output through a linear projection layer, followed by the denormalization and reshaping to yield the predicted CSI $\hat{\mathcal{Y}}_{pred} = \{\hat{\mathcal{H}}_{t+1},\dots,\hat{\mathcal{H}}_{t+L}\} \in \mathbb{C}^{L\times N_c\times N_r\times N_t}$.



\subsection{Training}
We form training samples by sliding a window \cite{Scaling-Law-for-Time-Series-Forecasting} of length $P+L$ over the time dimension of training dataset $\mathcal{D}_{train}$, splitting each window $\mathcal {X}=\mathcal{H}_{t-P+1:t+L}$ into a historical segment $\mathcal {X}_{his}=\mathcal{H}_{t-P+1:t}$ and a target segment $\mathcal {X}_{gt}=\mathcal{H}_{t:t+L}$. With $\mathcal {X}_{\mathrm{gt}}$ as supervision, we train the model to map $\mathcal {X}_{his}$ to the prediction $ \hat{\mathcal{Y}}_{pred}=\mathcal{H}_{t:t+L}$.

The total loss of our model is formed by:
\begin{equation}
    \mathcal{L}_{total} = \mathcal{L}_{pred} + \lambda \mathcal{L}_{rec},
\end{equation}
The $\mathcal {L}_{pred}$ minimizes the normalized mean squared error (MSE) between the predicted and ground-truth CSI:
\begin{equation}
    \mathcal{L}_{pred} = \frac{\|\hat{\mathcal{Y}}_{pred} - \mathcal{X}_{gt}\|_F^2}{\|\mathcal{X}_{gt}\|_F^2},
    \label{nmse}
\end{equation}
where $\|\cdot\|_F$ denotes the Frobenius norm. 
Inspired by the concept of Deep Supervision \cite{Lalign}, which employ discriminative classifiers for intermediate layers, we propose an auxiliary reconstruction objective $\mathcal{L}_{rec}$:
\begin{equation} 
    \mathcal{L}{rec} = \| \mathcal{R}(\mathbf{Z}_q \mathbf{W}_{rec} + \mathbf{b}_{rec}) - \mathcal{X}_{his} \|_F^2, 
\end{equation}
where $\|\cdot\|_F$ denotes the Frobenius norm, $\mathbf{Z}_q \in \mathbb{R}^{N_q \times D}$ denotes the output of the Q-Former, and $\mathcal{R}(\cdot)$ denotes the reshaping operation that restores the spatial-temporal dimensions subsequent to the affine transformation of $\mathbf{Z}_q$ using the weight matrix $\mathbf{W}_{rec}$ and bias vector $\mathbf{b}_{rec}$. The hyperparameter $\lambda$ balances the auxiliary loss $\mathcal{L}_{rec}$ with the primary loss $\mathcal{L}_{pred}$.



\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}
% -------- Table --------
\begin{table}[tbp]
\caption{Parameters for Dataset}
\begin{center}
\setlength{\tabcolsep}{15pt} % 增加列间距，因为只有三列，宽一点好看
\begin{tabular}{c|c}
\toprule
{\bfseries Parameters} & {\bfseries Value} \\
\midrule
Scenario & Dense Urban (Macro only) \\
Channel model & According to TR 38.901 \\
Inter-BS distance & 200m \\
Frequency Range & FR1 only; 2GHz \\
Subcarrier Spacing & 15kHz for 2GHz \\
Bandwidth & 10M (52RB) \\
Speed & 30/60/120/Mix.\ km/h \\
Data size & (21000, 20, 2, 32, 4, 8) \\
\bottomrule
\end{tabular}
\label{tab:scenario}
\end{center}
\end{table}
\subsubsection{Datasets}
We evaluated our model on the open mobile communication dataset\footnote{\url{www.mobileai-dataset.com}}, categorized into four subsets according to user velocity: 30km/h, 60km/h, 120km/h, and a mixture of samples from the aforementioned three speed levels. For each subset, we collect 21,000 samples structured as time-frequency grids across 32 transmit and 4 receive antennas. Each sample encompasses 20 time steps with a Transmission Time Interval (TTI) of 5 ms and spans 8 Physical Resource Blocks (PRBs) in the frequency domain. Specific simulation parameters are listed in Table~\ref{tab:scenario}.

\subsubsection{Baseline} 
To evaluate the performance of our proposed model, we compared it with existing methods, such as complex-valued convolutional neural networks (CVCNN) \cite{cp-cvcnn}, long short-term memory (LSTM) networks \cite{cp-rnn-2}, STEM GNN \cite{cp-stem-gnn}, and LLM4CP \cite{LLM4CP}. To ensure a fair comparison across all baselines, we adopt a unified experimental framework. 

\subsubsection{Evaluation Metrics}
We employ Normalized Mean Squared Error(NMSE) and Squared Generalized Cosine Similarity(SGCS), which are standard metrics for channel prediction~\cite{3gpp_38.843} to quantifies the numerical and spatial discrepancy between the predicted channel states and the ground truth, respectively. NMSE serves as the optimization objective in our framework, whose formulation is detailed in Eq.~\ref{nmse}.
% \begin{equation}
%     \mathrm{NMSE}=\frac{\|\widehat{\mathbf{H}}-\mathbf{H}\|_2^2}{\|\mathbf{H}\|_2^2},
% \end{equation}
% where $\mathbf{H} \in \mathbb{C}^{N_s \times N}$ and  $\hat{\mathbf{H}}$ represents the target channel matrix and the model output, respectively and $\| \cdot \|_2$ represents the Frobenius norm.
SGCS is defined as:
\begin{equation}
    \operatorname{SGCS} = \frac{1}{N_{sp}}\sum_{i=1}^{N_{sp}}\frac{1}{N_{rb}}\sum_{j=1}^{N_{rb}} \frac {\lVert \mathbf H_{i,j} \hat {\mathbf H}_{i,j} \rVert ^ 2} { \lVert \mathbf H_{i,j} \rVert ^2 \lVert\hat{\mathbf H}_{i,j} \rVert ^2},
\end{equation}
where $N_{sp}$ represents the number of samples, $N_{rb}$ is the number of resource blocks per sample, $\mathbf H_{i,j} \in \mathcal{C}^{N_r \times N_t}$ and $\hat{\mathbf H}_{i,j} \in \mathcal{C}^{N_r \times N_t}$ are the predicted channel matrices and ground truth, respectively. 

\subsection{Implementation Details}
This experiment was implemented on 4 NVIDIA RTX 4090 GPUs with 24 GB memory under Ubuntu 22.04.3 LTS environment. The AdamW \cite{Adamw} optimizer is used with an initial learning rate of 0.001, combined with a cosine annealing scheduler and a warm-up phase of 10\% epochs to adjust the learning rate. We adopted the pre-trained CLIP ViT-B/16 model~\cite{clip} as vision encoder and the pre-trained GPT-2~\cite{gpt2} as decoder-only large language model. The hyperparameter $\lambda$ was set to 0.1, and the sensitivity threshold $\eta$ in coherence segmentation was set to 0.05 based on validation performance. 

\begin{table*}[t]
\centering
\caption{The size of observation window is set as 12 and prediction window size $l_o \in \{2, 4, 8\}$. For NMSE, lower values indicate better performance, for SGCS, higher values indicate better performance. Bold: best, Underline: second best}
\label{tab:results}
\resizebox{0.9\textwidth}{!}{% <--- 开始缩放
\begin{tabular}{c|c|c@{  }c|c@{  }c|c@{  }c|c@{  }c|c@{  }c}
\toprule
\multicolumn{2}{c}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Ours}} & \multicolumn{2}{c}{\textbf{LSTM}} & \multicolumn{2}{c}{\textbf{CVCNN}} & \multicolumn{2}{c}{\textbf{STEM GNN}} & \multicolumn{2}{c}{\textbf{LLM4CP}} \\
\midrule

\multicolumn{2}{c}{\textbf{Metric}}  & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ \\

\midrule
\multirow{3}{*}{\rotatebox{90}{30km/h}}
 & 2 & \textbf{0.017} & \textbf{0.944} & 0.179 & 0.678 & 0.186 & 0.894 & 0.072 & \underline{0.927} & \underline{0.036} & 0.908 \\
 & 4 & \textbf{0.045} & \textbf{0.896} & 0.216 & 0.646 & 0.490 & 0.628 & 0.228 & 0.784 & \underline{0.072} & \underline{0.849} \\
 & 8 & \textbf{0.113} & \textbf{0.799} & 0.586 & 0.376 & 0.860 & 0.214 & 0.601 & 0.502 & \underline{0.148} & \underline{0.723} \\
\midrule
Average & - & \textbf{0.058} & \textbf{0.880} & 0.327 & 0.566 & 0.512 & 0.579 & 0.300 & 0.738 & \underline{0.086} & \underline{0.827} \\
\midrule
\multirow{3}{*}{\rotatebox{90}{60km/h}}
 & 2 & \textbf{0.136} & \textbf{0.740} & 0.201 & 0.655 & 0.846 & 0.176 & 0.518 & 0.435 & \underline{0.150} & \underline{0.716} \\
 & 4 & \textbf{0.151} & \textbf{0.718} & 0.212 & 0.650 & 0.883 & 0.177 & 0.471 & 0.491 & \underline{0.161} & \underline{0.693} \\
 & 8 & \textbf{0.174} & \textbf{0.680} & 0.641 & 0.340 & 0.959 & 0.118 & 0.672 & 0.335 & \underline{0.183} & \underline{0.666} \\
\midrule
Average & - & \textbf{0.131} & \textbf{0.752} & 0.417 & 0.506 & 0.831 & 0.254 & 0.489 & 0.508 & \underline{0.143} & \underline{0.730} \\
\midrule
\multirow{3}{*}{\rotatebox{90}{120km/h}}
 & 2 & \underline{0.163} & \underline{0.687} & 0.211 & 0.661 & 0.919 & 0.129 & 0.520 & 0.444 & \textbf{0.159} & \textbf{0.702} \\
 & 4 & \underline{0.167} & \underline{0.678} & 0.472 & 0.474 & 0.952 & 0.139 & 0.470 & 0.495 & \textbf{0.164} & \textbf{0.689} \\
 & 8 & \underline{0.190} & \underline{0.658} & 0.808 & 0.246 & 0.982 & 0.073 & 0.870 & 0.180 & \textbf{0.178} & \textbf{0.669} \\
\midrule
Average & - & \textbf{0.131} & \textbf{0.752} & 0.417 & 0.506 & 0.831 & 0.254 & 0.489 & 0.508 & \underline{0.143} & \underline{0.730} \\
\midrule
\multirow{3}{*}{\rotatebox{90}{x km/h}}
 & 2 & \textbf{0.116} & \textbf{0.785} & 0.202 & 0.659 & 0.948 & 0.224 & 0.350 & 0.619 & \underline{0.134} & \underline{0.749} \\
 & 4 & \textbf{0.138} & \textbf{0.748} & 0.483 & 0.447 & 0.960 & 0.200 & 0.300 & 0.635 & \underline{0.153} & \underline{0.708} \\
 & 8 & \textbf{0.164} & \textbf{0.694} & 0.793 & 0.243 & 0.984 & 0.078 & 0.798 & 0.255 & \underline{0.178} & \underline{0.680} \\
\midrule
Average & - & \textbf{0.131} & \textbf{0.752} & 0.417 & 0.506 & 0.831 & 0.254 & 0.489 & 0.508 & \underline{0.143} & \underline{0.730} \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Result}
The prediction window $P$ is set within \{2, 4, 8\}, while the look-back window size $L$ is set at 12 for all datasets. We calculate the NMSE and SGCS of baseline and our proposed model and show the results in TABLE~\ref{tab:results}. It can be observed that our proposed consistently outperforms most baseline methods across different speed scenarios and prediction lengths. Specifically, in low-mobility scenarios with user speed in 30 km/h, we achieves significant improvements in prediction window 8, increasing SGCS by up to 10.5\% compared to the second-best method. In dataset with user speed in 60 km/h and compelx-mobility scenarios(x km/h), our model maintains robust performance, but in high-moblity scenario(120 km/h).
This is because temporal models excel at capturing long-term evolutionary trends, while spatial models are adept at extracting instantaneous structural features. By leveraging a joint spatiotemporal modeling mechanism, our model ensures both robustness in high-dynamic settings and reconstruction precision in complex spatial environments, thereby achieving superior generalization.

\begin{table}[htbp]
\centering
\caption{Ablation study of key components. ``w/o'' denotes without.}
\label{tab:ablation_wo}
\setlength{\tabcolsep}{12pt} % 增加列间距，因为只有三列，宽一点好看
\begin{tabular}{l|cc}
\toprule
\textbf{Setting} & \textbf{NMSE}& \textbf{SGCS} \\
\midrule
\textbf{Ours (Full Model)} & \textbf{0.116} & \textbf{0.785} \\
\midrule
w/o Coherence Embedding(CE) & 0.123 & 0.774 \\
w/o R(2+1)DCVCNN & 0.324 & 0.613 \\
w/o CE \& R(2+1)DCVCNN & 0.329 & 0.610 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{The impact of threshold $\eta$ across all datasets in term of NMSE.}
\label{tab:ablation_hyper}
\setlength{\tabcolsep}{5pt} % 稍微收紧一点间距
\begin{tabular}{c|ccccc}
\toprule
% 左边是 Patch Size 的实验
\textbf{Threshold ($\eta $)} & 30km/h & 60km/h & 120km/h & Mix. \\
\midrule
0.01  & 0.120 & 0.176  & 0.192 & 0.169  \\
0.05  & \textbf{0.113} & 0.174  & \textbf{0.190} & \textbf{0.164}  \\
0.10  & 0.115 & \textbf{0.172}  & 0.194 & 0.170  \\
0.20 & 0.117 & 0.175  & 0.193 & 0.166  \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}
% To validate the effectiveness of the proposed spatial-structural guidance block and temporal-coherence guidance block, we conduct ablation studies by removing each module from the full framework. The results are presented in Table~\ref{tab:ablation_wo}. It can be observed that removing either the SSG or TCG block leads to a degradation in performance. Specifically, without the SSG block, the NMSE increases from 0.107 to 0.135, indicating a 26.2\% performance drop. Similarly, removing the TCG block results in an NMSE increase to 0.128, corresponding to a 19.6\% performance drop. These findings highlight the crucial role of both guidance blocks in enhancing the model's ability to capture spatial and temporal features of CSI data effectively.
To verify the effectiveness of the key components in our proposed framework, specifically the Coherence Embedding and the R(2+1)D CVCNN module, we conducted an ablation study on the mixed-velocity dataset, as summarized in Table~\ref{tab:ablation_wo}
The most significant performance degradation is observed when the R(2+1)D CVCNN module is removed. 
% As shown in the third row, the NMSE deteriorates drastically from 0.116 to 0.324, and SGCS drops from 0.785 to 0.613. 
indicating that the R(2+1)D CVCNN serves as the fundamental backbone of our system, playing a decisive role in extracting local spatiotemporal features from the CSI sequence. Without this module, the model fails to capture the intricate physical structures of the channel, leading to poor reconstruction capability. The omission of the Coherence Embedding leads to a noticeable decline in performance, with NMSE rising to 0.123. Although this impact is less drastic than removing the CNN backbone, it highlights the embedding's role in enforcing temporal alignment. By explicitly modeling the continuity of channel evolution, this component is essential for fine-tuning prediction accuracy. Furthermore, the full model's superior performance across all metrics—compared to the variant lacking both components—demonstrates that integrating spatiotemporal feature extraction (via R(2+1)D) with temporal coherence modeling is vital for achieving precise channel prediction

As shown in Table~\ref{tab:ablation_hyper}, we evaluate the NMSE performance under varying threshold values $\eta \in \{0.01, 0.05, 0.10, 0.20\}$.
We observe that the performance peaks at $\eta=0.05$, achieving the best NMSE of 0.113, 0.190, and 0.164 for the 30km/h, 120km/h, and Mixed datasets, respectively. Both lower ($\eta=0.01$) and higher ($\eta=0.20$) thresholds lead to increased prediction errors. Therefore, $\eta=0.05$ is selected as the optimal hyperparameter to balance feature retention and noise suppression.

\section{Conclusion}
\label{sec:conv}
In this work, we addressed the limitation of decoupled spatial and temporal modeling in existing studies by proposing a unified VLM-based framework. Through the synergistic integration of the Spatial-Structural and Temporal-Coherence Guidance Blocks, we successfully aligned the high-dimensional CSI features with the semantic reasoning capabilities of LLMs. Our empirical results demonstrate that this cross-modal alignment leads to significant gains in prediction accuracy and structural fidelity, particularly in complex, high-mobility scenarios. These outcomes highlight the promising potential of VLM-based model in wireless communication applications.

% \section{Introduction}
% % \label{sec:intro}

% These guidelines include complete descriptions of the fonts, spacing, and related information for producing your proceedings manuscripts. Please follow them. 

% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Paper length}

% Papers should be no longer than 6 pages, including all text, figures, and references.

% \subsection{Supplemental material}

% Authors may optionally upload supplemental material. Typically, this material might include:
% \begin{itemize}
%     \item a short presentation summarizing the paper,
%     \item videos of results that cannot be included in the main paper,
%     \item screen recording of the running program
%     \item anonymized related submissions to other conferences and journals, and
%     \item appendices or technical reports containing extended proofs and mathematical derivations that are not essential for understanding of the paper.
% \end{itemize}

% Note that the contents of the supplemental material should be referred to appropriately in the paper and that reviewers are not obliged to look at it.

% All supplemental material must be zipped into a single file. There is a 20MB limit on the size of this file.

% \subsection{Dual submission}

% By submitting a manuscript to ICME, the authors guarantee that it has not been previously published (or accepted for publication) in substantially similar form. Furthermore, no paper which contains significant overlap with the contributions of this paper either has been or will be submitted during the ICME 2026 review period to either a journal or a conference.

% If there are papers that may appear to violate any of these conditions, then it is the authors' responsibility to (1) cite these papers (preserving anonymity as described in Section 2 of this example paper), (2) argue in the body of your paper why your ICME paper is nontrivially different from these concurrent submissions, and (3) include anonymized versions of those papers in the supplemental material.

% \section{Double-Blind Review}

% Many authors misunderstand the concept of anonymizing for double-blind review. Double-blind review does not mean that one must remove citations to one's own work -- in fact it is often impossible to review a paper unless the previous citations are known and available.

% Double-blind review means that you do not put your name in the authors' list and do not use the words ``my'' or ``our'' when citing previous work. That is all. (But see below for technical reports)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith, it says that you are building on her work. If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]''and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of our
%    previous paper [1], and show it to be inferior to all
%    previously known methods. Why the previous paper was
%    accepted without this analysis is beyond me.

%    [1] Removed for blind review
% \end{quote}

% An example of an excellent paper:

% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of the
%    paper of Smith [1], and show it to be inferior to
%    all previously known methods.  Why the previous paper
%    was accepted without this analysis is beyond me.

%    [1] Smith, L and Jones, C. ``The frobnicatable foo
%    filter, a fundamental contribution to human knowledge''.
%    Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work. In such cases, include the anonymized parallel submission~\cite{Authors12} as additional material and cite it as

% \begin{quote}
% 1. Authors. ``The frobnicatable foo filter'', ACM MM 2016 Submission ID 324, Supplied as additional material {\tt acmmm13.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report. For conference
% submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a technical report for further details. Thus, you may say in
% the body of the paper ``further details may be found in~\cite{Authors12b}''.  Then submit the technical report as additional material. Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool which is widely known to be restricted to a single institution.  For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the ICME audience would like to hear about your solution.  The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus.

% You can handle this paper like any other.  Don't write ``We show how to improve our previous work [Anonymous, 1968].  This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''. That would be silly, and would immediately identify the authors. Instead write the following:
% \begin{quotation}
% \noindent
%    We describe a system for zero-g frobnication.  This
%    system is new because it handles the following cases:
%    A, B.  Previous systems [Zeus et al. 1968] didn't
%    handle case B properly.  Ours handles it by including
%    a foo term in the bar integral.

%    ...

%    The proposed system was integrated with the Apollo
%    lunar lander, and went all the way to the moon, don't
%    you know.  It displayed the following behaviours
%    which show how well we solved cases A and B: ...
% \end{quotation}

% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors. A reviewer might think it likely that the new paper was written by Zeus, but cannot make any decision based on that guess. He or she would have to be sure that no other authors could have been contracted to solve problem B.

% FAQ: Are acknowledgements OK?  No. Please omit them in the review copy and leave them for the final, camera ready copy.

% \section{Ease of Use}

% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma. \label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{Do not include authors' names or affiliations in the initial submission, since the review is double-blind.} A different template will be provided for camera-ready papers, which would allow authors' names and affiliations to be displayed. The class file is designed for, but not limited to, six authors. A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. \color{red}Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.
% \color{black}

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{./pic/fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.




% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\bibliographystyle{IEEEbib}
\bibliography{icme2026references}

\end{document}
