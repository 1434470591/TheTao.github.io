\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage{tikz}
\usepackage{url}  % 或 \usepackage{hyperref}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{makecell}
% 必备宏包
\usepackage{booktabs}  % 必须：提供三线表 (\toprule, \midrule, \bottomrule)
\usepackage{amssymb}   % 必须：提供基础数学符号 (对钩和叉号)
\usepackage{multirow}  % 可选：用于合并行
\usepackage{xcolor}    % 可选：用于灰底高亮

% 定义安全版符号 (不再依赖 pifont)
\newcommand{\cmark}{\checkmark}     % 使用数学字体的对钩
\newcommand{\xmark}{$\times$}       % 使用数学字体的乘号作为叉
\newcommand{\nmark}{--}             % 或者用横线表示未选用
\usetikzlibrary{positioning, calc}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Spatial-Temporal Channel Prediction via a Dual-Domain Augmented VLM-based Framework}

\author{Anonymous ICME submission}

\maketitle

\begin{abstract}
Channel state information (CSI) prediction is a crucial technology in future sixth-generation (6G) wireless communication systems. Although existing deep learning-based methods have achieved notable improvements in CSI prediction, they primarily focus on temporal or spectral feature extraction while neglecting spatial domain characteristics. This inherent limitation results in suboptimal performance when deployed in massive multi-input multi-output environments, where spatial correlation plays a critical role. Meanwhile, recent advances in vision-language models (VLMs) have enabled the extracting of complex spatial-temporal dependencies through multimodal representation learning. However, this paradigm remains unexplored in CSI prediction tasks. In this paper, we propose a novel dual-domain augmented VLM-based framework that explicitly integrates spatial-temporal coherence and spectral correlation for enhanced CSI estimation. Specifically, the architecture comprises two modules: a spatial-structural guidance block to extract geometric features from CSI data and a temporal-coherence guidance block to capture temporal cohenrence and physical consistency. Experimental results demonstrate that our proposed model outperforms the compared baseline methods in term of normalized mean squared error (NMSE) and squared generalized cosine similarity (SGCS), verifying its capability in capturing multi-dimensional channel structures.
% \cite{cp-cvcnn}\cite{vit}\cite{cp-ar}\cite{cp-kf}

% This document is a model and instructions for \LaTeX.
% This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
% or Math in Paper Title or Abstract. The abstract should contain about 100 to 150 words, and should be identical to the abstract text submitted electronically. 
\end{abstract}

\begin{IEEEkeywords}
Channel prediction, vision-language models, channel state information, multimodal learning, spatial-temporal modeling
\end{IEEEkeywords}


\section{Introduction}
\label{sec:intro}
% CSI的作用
% MIMO的作用
% MIMO对CP的影响
% 单模态的缺点
The accurate acquisition of channel state information (CSI) is crucial in communication systems, providing additional spatial gain for diversified wireless applications~\cite{DBLP:books/cu/TV2005}.  As shown in Figure~\ref{fig:data}, orthogonal pilot symbols embedded in the signal transmission frame are used as known reference signals for CSI estimation to obtain the unknown data. Multiple-input multiple-output (MIMO) technology leverages the spatial dimension of large antenna arrays to achieve significant multiplexing and diversity gains~\cite{MIMO}. However, with the the evolution towards massive MIMO arrays, the excessive pilot overhead poses a serve challenge to accurate CSI acquisition. This is because the required pilot resources scale with the explosive growth in the number of antennas. To resolve this, channel prediction has become a key technology to reduce the overhead and enhance system performance. By inferring future CSI from historical data, it enables the system to obtain channel information without dedicated pilot symbols.

Fueled by the rapid advancements in deep learning, data-driven methods have achieved substantial performance gains in wireless communication systems~\cite{cf-CEFnet,ECT-Net}, particularly in CSI prediction tasks. For instance, recurrent neural networks (RNNs) and long short-term memory (LSTM) have demonstrated powerful performance in channel prediction by effectively capturing dynamic temporal dependencies~\cite{cp-rnn-1, cp-rnn-2}. Studies on temporal feature modeling of CSI have been further advanced by Transformers~\cite{cp-transformer} and large language model (LLM)-based approaches~\cite{LLM4CP}, which excel at modeling long-range sequential correlations. 

Beyond temporal dependence, parallel efforts have focused on the spatial dependencies. Complex-valued convolution neural networks have been specifically designed to preserve the phase-amplitude relationships of complex-valued CSI~\cite{cp-cvcnn}. Researchers in~\cite{cp-stem-gnn} demonstrated that the superior spatial feature extraction capabilities of STEM GNN can be transferred to wireless channel modeling. WiFo~\cite{wifo} applies masked autoencoders (MAE)~\cite{MAE} to reconstruction tasks, treating CSI matrices as visual images to extract deep spatial structural features.

\begin{figure}[tb]
\centerline{\includegraphics{./pic/Task.pdf}}
% \includegraphics[width=\linewidth]{./pic/Task.pdf}
\caption{The data structure in MIMO-OFDM system. }
\label{fig:data}
\end{figure}

Despite these achievements, prior approaches fundamentally treats the spatial domain and temporal-spectral domains in isolation, ignoring the intrinsic spatiotemporal correlation of wireless channels. Specifically, above temporal-focused models simply flatten multi-dimensional CSI tensors into vectors, a process that inevitably obliterates the underlying spatial structure~\cite{cp-rnn-1, cp-rnn-2, cp-transformer, LLM4CP}. Conversely, spatial-focused models, while preserving structural features, often struggle to maintain precision across long prediction windows due to limited temporal modeling capabilities~\cite{cp-cvcnn, cp-stem-gnn, wifo}. To bridge this gap, we introduce a dual-domain augmented framework that leverages the pre-trained vision encoder and decoder-only LLMs to enhance spatial-temporal channel prediction. Our contributions are summarized as follows:

\begin{itemize}
    \item We propose VLM-based framework to enhance spatial-temporal channel prediction by leveraging the robust representational capabilities of pre-trained vision encoder and large language model.
    % 用VLM的多模态特性，提取block得到的特征，
    \item We employ a spatial-structural guidance block and a temporal-coherence guidance block to extract multi-dimensional features, which are then integrated as multimodal inputs to leverage the VLM's powerful cross-modal reasoning capabilities
    \item The proposed method is validated on several open-source datasets featuring multi-dimensional CSI. Extensive experiments demonstrate that it achieves superior performance in spatio-temporal channel prediction tasks, outperforming baselines in terms of normalized mean squared error (NMSE) and squared generalized cosine similarity (SGCS).
\end{itemize}

\section{Related work}
\label{sec:rew}

\subsection{Channel Prediction}
In the field of channel prediction, traditional algorithms like the autoregressive (AR) model~\cite{cp-ar} and the kalman filtering~\cite{cp-kf} are computationally efficient and straightforward to implement. Nevertheless, their accuracy relies heavily on linear assumptions and requires stationary environments. 

Related efforts in deep learning for channel prediction have followed two major research axes: temporal sequence modeling and spatial feature extraction. RNN and LSTM excel in channel prediction by capturing dynamic temporal features~\cite{cp-rnn-1, cp-rnn-2}. Additionally, Transformer-based models significantly advance channel prediction in scenarios with long prediction windows, by facilitating parallel computation~\cite{cp-transformer}. More recently, inspired by the advanced sequence-to-sequence modeling expertise of large language models in fields of natural language processing (NLP), several studies reflect their potential in CSI tasks. For instance, LLM4CP~\cite{LLM4CP} fine-tuned a pre-trained GPT-2~\cite{gpt2} for CSI data and deployed a set of modules to boost model effectiveness. Similarly, the authors in~\cite{LLM4CE} leveraged the powerful noise removal capability of LLM to improve CSI reconstruction performance. 

In parallel, CNN-based approaches employ convolutional operations to extract the spatial structure of CSI data~\cite{cp-cnn-1, cp-cnn-2}. Furthermore, CVCNN~\cite{cp-cvcnn} utilizes complex-valued layers to effectively preserve the phase-amplitude relationships of CSI, thus capturing richer spatial features than conventional real-valued networks. Similarly, STEM GNN~\cite{cp-stem-gnn} utilizes its specialized graph convolutional architecture to jointly capture latent spatial correlations. More recently, WiFo~\cite{wifo} utilizes a self-supervised learning paradigm by applying the masked autoencoder (MAE)~\cite{MAE} to reconstruct CSI images, thereby effectively capturing intricate spatial structural features for channel tasks. However, current architectures typically treat temporal dependence and spatial structures as independent entities, thereby overlooking the complex spatio-temporal correlations and the multi-dimensional synergy of the CSI.

\subsection{Vision-Language Models} 
Vision-language models (VLMs) advance multimodal learning by bridging the gap between visual and textual modalities
CLIP~\cite{clip} and ALIGN~\cite{align} demonstrate that contrastive learning effectively aligns image and text embeddings in a shared latent space. BLIP-2~\cite{blip-2} incorporate Query Transformer (Q-Former) with cross-attention mechanisms to improve cross-modal interaction. In contrast, LLaVA~\cite{llava} employs a lightweight interface by feeding visual features into the LLM via a single learnable linear projection layer.

Beyond the field of computer vision (CV), authors in~\cite{Time-VLM} have extended the application of VLMs to non-visual domains, transforming structured data into visual representations. Therefore VLMs are not limited to native images and can serve as universal spatial-temporal learners through cross-modal representation learning.
% Our goal is to bridge this gap by leveraging VLMs to combine CSI data, visual, and textual modalities. These advances demonstrate that VLMs are not limited to native images and can reinforce their potential to serve as universal cross-modal learners across diverse scientific and data-intensive tasks.


\section{Method}
\label{sec:method}

\subsection{Problem Formulation}
\label{subsec:problem_formulation}

As shown in Fig.~\ref{fig:data}, we consider a MIMO-OFDM system with $N_t$ transmit and $N_r$ receive antennas operating over $N_c$ subcarriers. At time step $t$ and subcarrier frequency $k \in \{1,\dots,N_c\}$, the received signal $\mathbf{y}_{t,k} \in \mathbb{C}^{N_r}$ is modeled as:
\begin{equation}
    \mathbf{y}_{t,k} = \mathbf{H}_{t,k} \mathbf{x}_{t,k} + \mathbf{n}_{t,k},
\label{eq:system_model}
\end{equation}
where $\mathbf{x}_{t,k} \in \mathbb{C}^{N_t}$ denotes the transmitted vector, and $\mathbf{n}_{t,k} \sim \mathcal{CN}(\mathbf{0}, \sigma^2\mathbf{I})$ represents the additive complex Gaussian noise. The term $\mathbf{H}_{t,k} \in \mathbb{C}^{N_r \times N_t}$ is the channel frequency response (CFR) matrix, the frequency-domain expression of CSI. Aggregating the channel matrices over all subcarriers, we represent the CSI snapshot at time $t$ as
$\mathcal{H}_t \triangleq \{\mathbf{H}_{t,1},\dots,\mathbf{H}_{t,N_c}\}$.
Standard schemes insert dense pilots in each coherence interval to estimate $\mathcal{H}_t$, which incurs substantial overhead and reduces spectral efficiency in large-scale or fast-varying channels. To mitigate this burden, we %exploit the temporal correlation of $\{\mathcal{H}_t\}$
formulate channel prediction as a time-series forecasting task, where a model with parameters $\boldsymbol{\Theta}$ maps $P$ historical CSI snapshots to $L$ future ones:
\begin{equation}
    \big(\widetilde{\mathcal{H}}_{t+1},\dots,\widetilde{\mathcal{H}}_{t+L}\big)
    = f_{\boldsymbol{\Theta}}\big(\mathcal{H}_{t-P+1},\dots,\mathcal{H}_{t}\big),
\end{equation}
where $\widetilde{\mathcal{H}}_{t+\ell}$ denotes the predicted CSI at time index $t+\ell$, $\ell = 1,\dots,L$.

\begin{figure*}[tb]
\centerline{\includegraphics{./pic/Framework.pdf}}
% \includegraphics[width=0.8\textwidth]{./pic/Framework.pdf}
\caption{Overview of our proposed method.}
\label{fig:Overview}
\end{figure*}

\subsection{Overall Architecture}

The overall architecture of our proposed model has been illustrated in Fig.~\ref{fig:Overview}, employing a VLM-based framework with frozen vision encoder and frozen decoder-only LLM. In order to enhance the extraction of spatial-temporal CSI features by the VLM, we introduce a dual-domain guidance mechanism: the spatial-structural guidance block and the temporal-coherence guidance block. The following sections provide a detailed description of each block.
% In order to improve the VLM’s extraction of CSI spatial-temporal features, we introduce a dual-guidance mechanism: spatial-structural guidance block and temporal-coherence guidance block. The following sections provide a detailed description of each block.

% These two modules independently extract features, SSG block utilizes the \textit{AR(2+1)D-CVC} to factorize spatial and temporal convolutions to extract geometric features, and the prompt-aware Quering Transformer, or Q-Former, to extract visual features from a frozen visual encoder. TCG block introduces the Coherence Embedding, unlike standard positional encodings, to capture the temporal continuity and physical consistency of the CSI sequence. Ultimately, the outputs of these two modules are fed into a frozen LLM for generative forecasting.

\subsubsection{Spatial-Structural Guidance}
The objective of this block is to extract geometric features of CSI data and extract visual features from frozen vision encoder.

Following research~\cite{cnn-before-vit} proving early convolutions improve visual transformer (ViT)~\cite{vit} optimization, we first process the historical input $\mathcal{X}_{\mathrm{his}}=\{\mathcal{H}_{t-P+1},\ldots,\mathcal{H}_{t}\} \in \mathbb{C}^{P\times N_c\times N_r\times N_t}$ through R(2+1)DCVCNN moudle. In this moudle, we utilize the residual complex-valued convolutional operations instead of standard CNN to explicitly preserve phase information, and then we factorize the standard 3D convolutional kernel size $3 \times 3 \times 3$ into $3 \times 3 \times 1$ and $1 \times 1 \times 3$ like study~\cite{R2+1D} to effectively capture the spatial correlations of $\mathcal{X}_{\mathrm{his}}$ between the transmit antennas and receive antennas and the temporal dynamics in the time dimension. The operations abave are executed as follows:
\begin{equation}
\begin{aligned}
\hat{\mathcal{X}} &= \operatorname{CVCNN}_{1D}(\operatorname{CVCNN}_{2D}(\mathcal{X}_{\mathrm{his}}, \mathbf{W}_S), \mathbf{W}_T) + \mathcal{X}_{\mathrm{his}}, \\
\end{aligned}
\end{equation}
where $\operatorname{CVCNN}_{2D}(\cdot, \mathbf{W}_S)$ and $\operatorname{CVCNN}_{1D}(\cdot, \mathbf{W}_T)$ denotes the complex-valued convolution with 2D kernel weights $\mathbf{W}_S$ and 1D kernel weights $\mathbf{W}_T$, respectively. Following this, the phase-amplitude representation $\hat{\mathcal{X}}$ is passed to the pre-trained ViT-based vision encoder, from where output visual embeddings $\mathbf{F}_{ve} \in \mathbb{R}^{N_v \times D_v}$. 

Unlike BLIP-2~\cite{blip-2}, we train querying transformer (Q-Former) from scratch. A set of $N_q$ learnable query embeddings $\mathbf{Q} \in \mathbb{R}^{N_q \times D_q}$ is initialized randomly and optimized during training. The instruction prompt $\mathbf{I}$ is designed
to guide the Q-Former to extract visual features relevant to
CSI prediction tasks. The visual embeddings $\mathbf{F}_{ve}$ is then align to the dimension of Q-Former $\hat{\mathbf{F}}_{ve} \in \mathbb{R}^{N_v \times D_q}$ after a linear layer.
% The input to the Q-Former contains the query embeddings $\mathbf{Q}$, the instruction prompt $\mathbf{I}$, and the visual embeddings $\mathbf{F}_{ve}$.
Through 6 layers of Q-Former, the earnable query embeddings interact with the instruction prompt $\mathbf{I}$ through bidirectional self-attention layers, and interact with visual features $\hat{\mathbf{F}}_{ve}$ to compress spatial information into instruction-aware visual representations through cross-attention layers. and then the fully connected feed-forward network (FFN)~\cite{Transformer}, consists of two linear transformations with a ReLU activation in between, is employed.
To this end, the operations within each $l$-th layers of Q-Former are executed as follows:
\begin{align}
\tilde{\mathbf{Q}}^{(l)} &= \mathbf{Q}^{(l-1)} + \operatorname{Attention}\Big(\operatorname{LN}([\mathbf{Q}^{(l-1)}, \mathbf{I}])\Big),\\
\hat{\mathbf{Q}}^{(l)} &= \tilde{\mathbf{Q}}^{(l)} + \operatorname{Attention}\Big(\operatorname{LN}(\tilde{\mathbf{Q}}^{(l)}), \hat{\mathbf{F}}_{ve}, \hat{\mathbf{F}}_{ve}\Big),\\
\mathbf{Q}^{(l)} &= \hat{\mathbf{Q}}^{(l)} + \operatorname{max}\Big(0, \operatorname{LN}(\hat{\mathbf{Q}}^{(l)})W_1 + b_1\Big)W_2 + b_2,
\end{align}
where $\operatorname{LN}(\cdot)$ denotes layer normalization~\cite{Transformer}, $[\cdot, \cdot]$ denotes the concatenation operator along the sequence dimension, $\mathbf{Q}^{(l)} $ represents the output of the $l$-th layer.
The fundamental operation of this layers is the Scaled Dot-Product Attention~\cite{Transformer}:
% , which computes the correlation weight between the query($Q$) and key($K$) and then uses this weight to perform a weighted aggregation of the value ($V$) as follows:
% 公式表达书
\begin{equation} 
    \operatorname{Attention}(Q, K, V) = \operatorname{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V. 
\end{equation}
Thus we yield the Q-Former output $\mathbf{Z}_q \in \mathbb{R}^{N_q \times D_q} = \mathbf{Q}^{(6)}$.
% \begin{equation}
%     \mathbf{F}_{vv} = \operatorname{Q-Former}(\mathbf{Q}, \mathbf{F}_{ve}, prompt)
% \end{equation}
% These vectors $\mathbf{F}_{vv}$ serve as soft prompt input to the frozen LLM.

% \begin{algorithm}[tb]
%    \caption{Complex Rate-Adaptive Coherence Segmentation (CRACS)}
%    \label{alg:cracs}
% \begin{algorithmic}[1]
%    \STATE {\bfseries Input:} Complex CSI sequence $\mathbf{H} \in \mathbb{C}^{T \times D}$, Sensitivity threshold $\tau \in \mathbb{R}^+$
%    \STATE {\bfseries Output:} Coherence Segment Indices $\mathbf{S} \in \mathbb{Z}^{T}$
   
%    \vspace{0.1cm}
%    \STATE \textcolor{gray}{\# Step 1: Compute Complex Differential Magnitude (The Rate)}
%    \STATE Initialize rate vector $\boldsymbol{\rho} \in \mathbb{R}^{T}$
%    \FOR{$t=2$ {\bfseries to} $T$}
%        \STATE \textcolor{gray}{\# Euclidean distance in complex plane: $\sqrt{\Delta Re^2 + \Delta Im^2}$}
%        \STATE $\boldsymbol{\rho}_t \leftarrow \| \mathbf{H}_t - \mathbf{H}_{t-1} \|_2$
%    \ENDFOR
%    \STATE $\boldsymbol{\rho}_1 \leftarrow \boldsymbol{\rho}_2$ \hfill \textcolor{gray}{\# Padding for the first step}
   
%    \vspace{0.1cm}
%    \STATE \textcolor{gray}{\# Step 2: Backward Recursive Clustering}
%    \STATE Initialize $\mathbf{S}$ with zeros; $id \leftarrow 0$; $\mathbf{S}[T] \leftarrow 0$
   
%    \FOR{$t=T$ {\bfseries down to} $2$}
%        \STATE \textcolor{gray}{\# Calculate ratio of current rate to preceding rate}
%        \STATE $ratio \leftarrow \frac{\boldsymbol{\rho}_t}{\boldsymbol{\rho}_{t-1} + \epsilon}$
       
%        \IF{$ratio < \tau$}
%            \STATE \textcolor{gray}{\# Stable evolution detected: Maintain Coherence ID}
%            \STATE $\mathbf{S}[t-1] \leftarrow id$
%        \ELSE
%            \STATE \textcolor{gray}{\# Volatility spike detected: Assign New ID}
%            \STATE $id \leftarrow id + 1$
%            \STATE $\mathbf{S}[t-1] \leftarrow id$
%        \ENDIF
%    \ENDFOR
   
%    \RETURN $\mathbf{S}$
% \end{algorithmic}
% \end{algorithm}

\subsubsection{Temporal-Coherence Guidance}The objective of this block is to calculate coherence embedding of CSI data and feed multi-input into frozen LLM.

We introduce the coherence embedding as the primary mechanism of this block, motivated by the theoretical channel correlation properties~\cite{TcEmbedding} and study~\cite{TcEmbedding}. The calculation method for coherence segmentation $\mathbf{S \in \mathbb{Z}}^{P}$ can be summarized by algorithm~\ref{alg:Coherence}. We calculate the coherence embedding $\mathbf{E}_{coh} \in \mathbb{R}^{P \times D_l}$ by mapping the coherence segmentation $\mathbf{S}$ through a learnable embedding layer $\boldsymbol{Embedding}(\cdot)$, where $D_l$ denotes the dimension of LLM.


\begin{algorithm}[tb]
   \caption{Coherence Segmentation}
   \label{alg:Coherence}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Historical CSI sequence $\mathbf{X} \in \mathbb{C}^{P \times D}$, threshold $\eta$
   \STATE {\bfseries Output:} Coherence Segment Array $\mathbf{S} \in \mathbb{Z}^{P}$

   \vspace{0.1cm}

   \STATE $current\_id \leftarrow 0$
   \STATE $\mathbf{S}[1] \leftarrow current\_id$
   \FOR{$t=2$ {\bfseries to} $T$}
    %    \STATE \textcolor{gray}{\# Calculate change ratio relative to the preceding step}
        \STATE $\boldsymbol{\delta} \leftarrow \| \mathbf{X}_t - \mathbf{X}_{t-1} \|_2$ 
\vspace{0.10cm}
       \STATE $ratio \leftarrow \dfrac{\boldsymbol{\delta}}{ \| \mathbf{X}_{t-1} \|_2 + \epsilon}$
\vspace{0.10cm}
       \IF{$ratio < \eta $}
        %    \STATE \textcolor{gray}{\# Coherence maintained: Assign same ID}
           \STATE $\mathbf{S}[t] \leftarrow current\_id$
       \ELSE
        %    \STATE \textcolor{gray}{\# Coherence break detected: Increment ID}
           \STATE $\mathbf{S}[t] \leftarrow current\_id + 1$

           \STATE $current\_id \leftarrow current\_id + 1$
       \ENDIF
   \ENDFOR
   
   \RETURN $\mathbf{S}$
\end{algorithmic}
\end{algorithm}


We first reshape original historical input $\mathcal {X}_{his} \in \mathbb{C}^{P\times N_c\times N_r\times N_t}$ into a dense real-valued representation $\mathcal X_{rearrange} \in \mathbb{R}^{P\times (2 N_c N_r N_t)}$, where the real and imaginary parts of each complex entry are decomposed and concatenated, and then we apply instance normalization~\cite{norm} to standardize each sample to zero mean and unit variance.

Thus, the composite embedding input $\mathbf{H}_{in}$ for frozen LLM can be represented as:
\begin{equation}
    \mathbf{H}_{in} = [\mathbf{Z}_q, \mathcal X_{rearrange} + \mathbf{E}_{coh}] + \mathbf{E}_{pos} + \mathbf{E}_{mod} ,
\end{equation}
where $\mathbf{E}_{coh}$ represents our proposed coherence embedding, $[\cdot, \cdot]$ denotes the concatenation operator along the sequence dimension, and $\mathbf{E}_{mod}$ and $\mathbf{E}_{pos}$ denote standard modal-type and positional embeddings in research~\cite{Vilt}.
% is a learnable coherence embedding vector designed to guide the frozen LLM in maintaining the temporal causality and physical consistency of the predicted CSI series.
The LLM processes the composite embedding input $\mathbf{H}_{in}$ to generate output hidden states that encode future channel dynamics. To map these semantic representations back to the physical CSI space, we pass the output through a linear projection layer, followed by the denormalization and reshaping to yield the predicted output $\hat{\mathcal{Y}}_{pred} = \{\hat{\mathcal{H}}_{t+1},\dots,\hat{\mathcal{H}}_{t+L}\} \in \mathbb{C}^{L\times N_c\times N_r\times N_t}$.

\subsection{Training}
We form training samples by sliding a window~\cite{Scaling-Law-for-Time-Series-Forecasting} of length $P+L$ over the time dimension of training dataset $\mathcal{D}_{train}$, splitting each window $\mathcal {X}=\mathcal{H}_{t-P+1:t+L}$ into a historical segment $\mathcal {X}_{his}=\mathcal{H}_{t-P+1:t}$ and a target segment $\mathcal {X}_{gt}=\mathcal{H}_{t:t+L}$. With $\mathcal {X}_{\mathrm{gt}}$ as supervision, we train the model to map $\mathcal {X}_{his}$ to the prediction $ \hat{\mathcal{Y}}_{pred}=\mathcal{H}_{t:t+L}$.

The total loss of our model is formed by:
\begin{equation}
    \mathcal{L}_{total} = \mathcal{L}_{pred} + \lambda \mathcal{L}_{rec}.
\end{equation}
The $\mathcal {L}_{pred}$ minimizes the normalized mean squared error (MSE) between the predicted and ground-truth CSI:
\begin{equation}
    \mathcal{L}_{pred} = \frac{\|\hat{\mathcal{Y}}_{pred} - \mathcal{X}_{gt}\|_F^2}{\|\mathcal{X}_{gt}\|_F^2},
    \label{nmse}
\end{equation}
where $\|\cdot\|_F$ denotes the Frobenius norm. 
Inspired by the concept~\cite{Lalign}, which employ discriminative classifiers for intermediate layers, we propose an auxiliary reconstruction objective $\mathcal{L}_{rec}$:
\begin{equation} 
    \mathcal{L}{rec} = \| \mathcal{R}(\mathbf{Z}_q \mathbf{W}_{rec} + \mathbf{b}_{rec}) - \mathcal{X}_{his} \|_F^2, 
\end{equation}
where $\|\cdot\|_F$ denotes the Frobenius norm, $\mathbf{Z}_q \in \mathbb{R}^{N_q \times D_quantity}$ denotes the output of the Q-Former, and $\mathcal{R}(\cdot)$ denotes the reshaping operation that restores the spatial-temporal dimensions subsequent to the affine transformation of $\mathbf{Z}_q$ using the weight matrix $\mathbf{W}_{rec}$ and bias vector $\mathbf{b}_{rec}$. The hyperparameter $\lambda$ balances the auxiliary loss $\mathcal{L}_{rec}$ with the primary loss $\mathcal{L}_{pred}$.



\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\subsubsection{Datasets}
We evaluated our model on the open mobile communication dataset\footnote{\url{www.mobileai-dataset.com}}, categorized into four subsets according to user velocity: 30km/h, 60km/h, 120km/h, and a mixture of samples from the aforementioned three speed levels. For each subset, we collect 21,000 samples structured as time-frequency grids across 32 transmit and 4 receive antennas. Each sample encompasses 20 time steps with a transmission time interval (TTI) of 5 ms and spans 8 physical resource blocks (PRBs) in the frequency domain. Specific simulation parameters are listed in Table~\ref{tab:scenario}.

\begin{table}[htbp]
\caption{Parameters for Dataset}
\begin{center}
\setlength{\tabcolsep}{15pt} % 增加列间距，因为只有三列，宽一点好看
\begin{tabular}{c|c}
\toprule
{\bfseries Parameters} & {\bfseries Value} \\
\midrule
Scenario & Dense Urban (Macro only) \\
Channel Model & According to TR 38.901 \\
Inter-BS Distance & 200m \\
Frequency Range & FR1 only; 2GHz \\
Subcarrier Spacing & 15kHz for 2GHz \\
Bandwidth & 10M (52RB) \\
Speed & 30/60/120/Mix.\ km/h \\
Data Size & (21000, 20, 2, 32, 4, 8) \\
\bottomrule
\end{tabular}
\label{tab:scenario}
\end{center}
\end{table}


\begin{table*}[t]
\centering
\caption{The size of observation window is set as 12 and prediction window size $l_o \in \{2, 4, 8\}$. For NMSE, lower values indicate better performance, for SGCS, higher values indicate better performance. Bold: best, Underline: second best}
\label{tab:results}
\resizebox{0.9\textwidth}{!}{% <--- 开始缩放
\begin{tabular}{c|c|c@{  }c|c@{  }c|c@{  }c|c@{  }c|c@{  }c}
\toprule
\multicolumn{2}{c}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Ours}} & \multicolumn{2}{c}{\textbf{LSTM}} & \multicolumn{2}{c}{\textbf{CVCNN}} & \multicolumn{2}{c}{\textbf{STEM GNN}} & \multicolumn{2}{c}{\textbf{LLM4CP}} \\
\midrule

\multicolumn{2}{c}{\textbf{Metric}}  & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ & \textbf{NMSE} $\downarrow$ & \textbf{SGCS} $\uparrow$ \\

\midrule
\multirow{3}{*}{\rotatebox{90}{30km/h}}
 & 2 & \textbf{0.017} & \textbf{0.944} & 0.179 & 0.678 & 0.186 & 0.894 & 0.072 & \underline{0.927} & \underline{0.036} & 0.908 \\
 & 4 & \textbf{0.045} & \textbf{0.896} & 0.216 & 0.646 & 0.490 & 0.628 & 0.228 & 0.784 & \underline{0.072} & \underline{0.849} \\
 & 8 & \textbf{0.113} & \textbf{0.799} & 0.586 & 0.376 & 0.860 & 0.214 & 0.601 & 0.502 & \underline{0.148} & \underline{0.723} \\
\midrule
Average & - & \textbf{0.058} & \textbf{0.880} & 0.327 & 0.566 & 0.512 & 0.579 & 0.300 & 0.738 & \underline{0.086} & \underline{0.827} \\
\midrule
\multirow{3}{*}{\rotatebox{90}{60km/h}}
 & 2 & \textbf{0.136} & \textbf{0.740} & 0.201 & 0.655 & 0.846 & 0.176 & 0.518 & 0.435 & \underline{0.150} & \underline{0.716} \\
 & 4 & \textbf{0.151} & \textbf{0.718} & 0.212 & 0.650 & 0.883 & 0.177 & 0.471 & 0.491 & \underline{0.161} & \underline{0.693} \\
 & 8 & \textbf{0.174} & \textbf{0.680} & 0.641 & 0.340 & 0.959 & 0.118 & 0.672 & 0.335 & \underline{0.183} & \underline{0.666} \\
\midrule
Average & - & \textbf{0.154} & \textbf{0.713} & 0.351 & 0.548 & 0.896 & 0.157 & 0.554 & 0.420 & \underline{0.165} & \underline{0.692} \\
\midrule
\multirow{3}{*}{\rotatebox{90}{120km/h}}
 & 2 & \underline{0.163} & \underline{0.687} & 0.211 & 0.661 & 0.919 & 0.129 & 0.520 & 0.444 & \textbf{0.159} & \textbf{0.702} \\
 & 4 & \underline{0.167} & \underline{0.678} & 0.472 & 0.474 & 0.952 & 0.139 & 0.470 & 0.495 & \textbf{0.164} & \textbf{0.689} \\
 & 8 & \underline{0.190} & \underline{0.658} & 0.808 & 0.246 & 0.982 & 0.073 & 0.870 & 0.180 & \textbf{0.178} & \textbf{0.669} \\
\midrule
Average & - & \underline{0.173} & \underline{0.674} & 0.497 & 0.460 & 0.951 & 0.114 & 0.620 & 0.373 & \textbf{0.167} & \textbf{0.686} \\
\midrule
\multirow{3}{*}{\rotatebox{90}{x km/h}}
 & 2 & \textbf{0.116} & \textbf{0.785} & 0.202 & 0.659 & 0.948 & 0.224 & 0.350 & 0.619 & \underline{0.134} & \underline{0.749} \\
 & 4 & \textbf{0.138} & \textbf{0.748} & 0.483 & 0.447 & 0.960 & 0.200 & 0.300 & 0.635 & \underline{0.153} & \underline{0.708} \\
 & 8 & \textbf{0.164} & \textbf{0.694} & 0.793 & 0.243 & 0.984 & 0.078 & 0.798 & 0.255 & \underline{0.178} & \underline{0.680} \\
\midrule
Average & - & \textbf{0.139} & \textbf{0.743} & 0.492 & 0.450 & 0.964 & 0.167 & 0.483 & 0.503 & \underline{0.155} & \underline{0.712} \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsubsection{Baseline} 
To evaluate the performance of our proposed model, we compare it with existing methods, such as  CVCNN~\cite{cp-cvcnn}, LSTM~\cite{cp-rnn-2}, STEM GNN~\cite{cp-stem-gnn}, and LLM4CP~\cite{LLM4CP}. To ensure a fair comparison across all baselines, we adopt a unified experimental framework. 

\subsubsection{Evaluation Metrics}
We employ normalized mean squared error(NMSE) and squared generalized cosine similarity(SGCS), which are standard metrics for channel prediction to quantifies the numerical and spatial discrepancy between the predicted channel states and the ground truth, respectively~\cite{3gpp_38.843}. NMSE serves as the optimization objective in our framework, whose formulation is detailed in Eq.~\ref{nmse}.
% \begin{equation}
%     \mathrm{NMSE}=\frac{\|\widehat{\mathbf{H}}-\mathbf{H}\|_2^2}{\|\mathbf{H}\|_2^2},
% \end{equation}
% where $\mathbf{H} \in \mathbb{C}^{N_s \times N}$ and  $\hat{\mathbf{H}}$ represents the target channel matrix and the model output, respectively and $\| \cdot \|_2$ represents the Frobenius norm.
SGCS is defined as:
\begin{equation}
    \operatorname{SGCS} = \frac{1}{N_{sp}}\sum_{i=1}^{N_{sp}}\frac{1}{N_{rb}}\sum_{j=1}^{N_{rb}} \frac {\lVert \mathbf H_{i,j} \hat {\mathbf H}_{i,j} \rVert ^ 2} { \lVert \mathbf H_{i,j} \rVert ^2 \lVert\hat{\mathbf H}_{i,j} \rVert ^2},
\end{equation}
where $N_{sp}$ represents the number of samples, $N_{rb}$ is the number of resource blocks per sample, $\mathbf H_{i,j} \in \mathcal{C}^{N_r \times N_t}$ and $\hat{\mathbf H}_{i,j} \in \mathcal{C}^{N_r \times N_t}$ are the predicted channel matrices and ground truth, respectively. 

\subsection{Implementation Details}
This experiment was implemented on 4 NVIDIA RTX 4090 GPUs with 24 GB memory under Ubuntu 22.04.3 LTS environment. The AdamW~\cite{Adamw} optimizer is used with an initial learning rate of 0.001, combined with a cosine annealing scheduler and a warm-up phase of 10\% epochs to adjust the learning rate. We adopted the pre-trained CLIP ViT-B/16 model~\cite{clip} as vision encoder and the pre-trained GPT-2~\cite{gpt2} as decoder-only large language model. The hyperparameter $\lambda$ was set to 0.1, and the sensitivity threshold $\eta$ in coherence segmentation was set to 0.05 based on validation performance. 


\subsection{Results}
The prediction window $P$ is set within \{2, 4, 8\}, while the look-back window size $L$ is set at 12 for all datasets. We calculate the NMSE and SGCS of baseline and our proposed model and show the results in TABLE~\ref{tab:results}. It can be observed that our proposed consistently outperforms most baseline methods across different speed scenarios and prediction lengths. Specifically, in low-mobility scenarios with user speed in 30 km/h, we achieves significant improvements in prediction window 8, increasing SGCS by up to 10.5\% compared to the second-best method. In dataset with user speed in 60 km/h and  mixed-velocity scenarios, our model maintains robust performance, but in high-velocity scenario it is marginally outperformed by LLM4CP.
This is because temporal models excel at capturing long-term evolutionary trends, while spatial models are adept at extracting instantaneous structural features. By leveraging a joint spatial-temporal modeling mechanism, our model ensures both reconstruction precision and robustness in high-dynamic settings in complex spatial environments, thereby achieving superior generalization.

\begin{table}[htbp]
\centering
\caption{Ablation study of key components. ``w/o'' denotes without.}
\label{tab:ablation_wo}
\setlength{\tabcolsep}{12pt} % 增加列间距，因为只有三列，宽一点好看
\begin{tabular}{l|cc}
\toprule
\textbf{Setting} & \textbf{NMSE}& \textbf{SGCS} \\
\midrule
\textbf{Ours (Full Model)} & \textbf{0.116} & \textbf{0.785} \\
\midrule
w/o coherence embedding (CE) & 0.123 & 0.774 \\
w/o R(2+1)DCVCNN & 0.324 & 0.613 \\
w/o CE \& R(2+1)DCVCNN & 0.329 & 0.610 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}
% To validate the effectiveness of the proposed spatial-structural guidance block and temporal-coherence guidance block, we conduct ablation studies by removing each module from the full framework. The results are presented in Table~\ref{tab:ablation_wo}. It can be observed that removing either the SSG or TCG block leads to a degradation in performance. Specifically, without the SSG block, the NMSE increases from 0.107 to 0.135, indicating a 26.2\% performance drop. Similarly, removing the TCG block results in an NMSE increase to 0.128, corresponding to a 19.6\% performance drop. These findings highlight the crucial role of both guidance blocks in enhancing the model's ability to capture spatial and temporal features of CSI data effectively.
To verify the effectiveness of the key components in our proposed framework, specifically the coherence embedding and the R(2+1)DCVCNN module, we conducted an ablation study on the mixed-velocity dataset, as summarized in Table~\ref{tab:ablation_wo}. 

The most significant performance degradation is observed when the R(2+1)DCVCNN module is removed. Thus this module serves as the fundamental component of our system, playing a decisive role in extracting comprehensive spatial features from the CSI. Without this module, the model fails to capture the phase-amplitude features of CSI, leading to poor prediction capability.
The exclusion of the coherence embedding leads to a slight performance degradation, with NMSE rising to 0.123. Although this impact is less drastic than removing the R(2+1)DCVCNN module, it underscores this embedding as critical role in enforcing temporal consistency. 
Ultimately, the superior performance of full model across all metrics, 
compared to the variant lacking both components—demonstrates,  integrates that the synergy between spatial feature extraction and temporal coherence modeling is vital for achieving channel prediction.
% As shown in Table~\ref{tab:ablation_hyper}, we evaluate the NMSE performance under varying threshold values $\eta \in \{0.01, 0.05, 0.10, 0.20\}$.
% We observe that the performance peaks at $\eta=0.05$, and both lower ($\eta=0.01$) and higher ($\eta=0.20$) thresholds lead to increased prediction errors. Therefore, $\eta=0.05$ is selected as the optimal hyperparameter to balance feature retention and noise suppression.




% \begin{table}[htbp]
% \centering
% \caption{The impact of threshold $\eta$ across all datasets in term of NMSE.}
% \label{tab:ablation_hyper}
% \setlength{\tabcolsep}{5pt} % 稍微收紧一点间距
% \begin{tabular}{c|ccccc}
% \toprule
% % 左边是 Patch Size 的实验
% \textbf{Threshold ($\eta $)} & 30km/h & 60km/h & 120km/h & Mix. \\
% \midrule
% 0.01  & 0.120 & 0.176  & 0.192 & 0.169  \\
% 0.05  & \textbf{0.113} & 0.174  & \textbf{0.190} & \textbf{0.164}  \\
% 0.10  & 0.115 & \textbf{0.172}  & 0.194 & 0.170  \\
% 0.20 & 0.117 & 0.175  & 0.193 & 0.166  \\
% \bottomrule
% \end{tabular}
% \end{table}

\section{Conclusion}
\label{sec:conv}
In this work, we addressed the limitation of decoupled spatial and temporal modeling in existing studies by proposing a dual-domain augmented VLM-based framework for spatial-temporal channel prediction. Through the integration of the spatial-structural and temporal-coherence guidance blocks, we ingeste multi-dimensional CSI snapshots in both original tensor and flattened formats. Our empirical results demonstrate that this cross-modal alignment leads to significant gains in prediction accuracy, particularly in complex-mobility scenarios. These outcomes highlight the promising potential of VLM-based model in wireless communication applications.

% \section{Introduction}
% % \label{sec:intro}

% These guidelines include complete descriptions of the fonts, spacing, and related information for producing your proceedings manuscripts. Please follow them. 

% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Paper length}

% Papers should be no longer than 6 pages, including all text, figures, and references.

% \subsection{Supplemental material}

% Authors may optionally upload supplemental material. Typically, this material might include:
% \begin{itemize}
%     \item a short presentation summarizing the paper,
%     \item videos of results that cannot be included in the main paper,
%     \item screen recording of the running program
%     \item anonymized related submissions to other conferences and journals, and
%     \item appendices or technical reports containing extended proofs and mathematical derivations that are not essential for understanding of the paper.
% \end{itemize}

% Note that the contents of the supplemental material should be referred to appropriately in the paper and that reviewers are not obliged to look at it.

% All supplemental material must be zipped into a single file. There is a 20MB limit on the size of this file.

% \subsection{Dual submission}

% By submitting a manuscript to ICME, the authors guarantee that it has not been previously published (or accepted for publication) in substantially similar form. Furthermore, no paper which contains significant overlap with the contributions of this paper either has been or will be submitted during the ICME 2026 review period to either a journal or a conference.

% If there are papers that may appear to violate any of these conditions, then it is the authors' responsibility to (1) cite these papers (preserving anonymity as described in Section 2 of this example paper), (2) argue in the body of your paper why your ICME paper is nontrivially different from these concurrent submissions, and (3) include anonymized versions of those papers in the supplemental material.

% \section{Double-Blind Review}

% Many authors misunderstand the concept of anonymizing for double-blind review. Double-blind review does not mean that one must remove citations to one's own work -- in fact it is often impossible to review a paper unless the previous citations are known and available.

% Double-blind review means that you do not put your name in the authors' list and do not use the words ``my'' or ``our'' when citing previous work. That is all. (But see below for technical reports)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith, it says that you are building on her work. If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]''and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of our
%    previous paper [1], and show it to be inferior to all
%    previously known methods. Why the previous paper was
%    accepted without this analysis is beyond me.

%    [1] Removed for blind review
% \end{quote}

% An example of an excellent paper:

% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of the
%    paper of Smith [1], and show it to be inferior to
%    all previously known methods.  Why the previous paper
%    was accepted without this analysis is beyond me.

%    [1] Smith, L and Jones, C. ``The frobnicatable foo
%    filter, a fundamental contribution to human knowledge''.
%    Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work. In such cases, include the anonymized parallel submission~\cite{Authors12} as additional material and cite it as

% \begin{quote}
% 1. Authors. ``The frobnicatable foo filter'', ACM MM 2016 Submission ID 324, Supplied as additional material {\tt acmmm13.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report. For conference
% submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a technical report for further details. Thus, you may say in
% the body of the paper ``further details may be found in~\cite{Authors12b}''.  Then submit the technical report as additional material. Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool which is widely known to be restricted to a single institution.  For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the ICME audience would like to hear about your solution.  The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus.

% You can handle this paper like any other.  Don't write ``We show how to improve our previous work [Anonymous, 1968].  This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''. That would be silly, and would immediately identify the authors. Instead write the following:
% \begin{quotation}
% \noindent
%    We describe a system for zero-g frobnication.  This
%    system is new because it handles the following cases:
%    A, B.  Previous systems [Zeus et al. 1968] didn't
%    handle case B properly.  Ours handles it by including
%    a foo term in the bar integral.

%    ...

%    The proposed system was integrated with the Apollo
%    lunar lander, and went all the way to the moon, don't
%    you know.  It displayed the following behaviours
%    which show how well we solved cases A and B: ...
% \end{quotation}

% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors. A reviewer might think it likely that the new paper was written by Zeus, but cannot make any decision based on that guess. He or she would have to be sure that no other authors could have been contracted to solve problem B.

% FAQ: Are acknowledgements OK?  No. Please omit them in the review copy and leave them for the final, camera ready copy.

% \section{Ease of Use}

% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma. \label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{Do not include authors' names or affiliations in the initial submission, since the review is double-blind.} A different template will be provided for camera-ready papers, which would allow authors' names and affiliations to be displayed. The class file is designed for, but not limited to, six authors. A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. \color{red}Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.
% \color{black}

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{./pic/fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.




% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\bibliographystyle{IEEEbib}
\bibliography{icme2026references}

\end{document}
